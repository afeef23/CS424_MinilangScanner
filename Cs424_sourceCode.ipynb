{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkIiAgD14OGP",
        "outputId": "8db504f5-5316-4b59-d5b4-7375e0a40db4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the filename: input.minilang\n",
            "Identifier: x\n",
            "Operator: =\n",
            "IntegerLiteral: 10\n",
            "Identifier: if\n",
            "Identifier: x\n",
            "Operator: ==\n",
            "IntegerLiteral: 10\n",
            "Identifier: print\n",
            "StringLiteral: \"x is 10\"\n",
            "Identifier: else\n",
            "Identifier: print\n",
            "StringLiteral: \"x is not 10\"\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "class TokenType:\n",
        "    IntegerLiteral = \"IntegerLiteral\"\n",
        "    BooleanLiteral = \"BooleanLiteral\"\n",
        "    Identifier = \"Identifier\"\n",
        "    Operator = \"Operator\"\n",
        "    Keyword = \"Keyword\"\n",
        "    Comment = \"Comment\"\n",
        "    StringLiteral = \"StringLiteral\"  # New token type for strings\n",
        "    InvalidToken = \"InvalidToken\"\n",
        "\n",
        "class Token:\n",
        "    def __init__(self, token_type, lexeme):\n",
        "        self.token_type = token_type\n",
        "        self.lexeme = lexeme\n",
        "\n",
        "class Scanner:\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        self.tokens = []\n",
        "\n",
        "    def scan(self):\n",
        "        with open(self.filename, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "            for line_number, line in enumerate(lines):\n",
        "                line = line.strip()\n",
        "                while line:\n",
        "                    match = None\n",
        "                    for pattern, token_type in self.patterns.items():\n",
        "                        match = re.match(pattern, line)\n",
        "                        if match:\n",
        "                            lexeme = match.group(0)\n",
        "                            line = line[len(lexeme):].strip()\n",
        "                            if token_type != TokenType.Comment:\n",
        "                                self.tokens.append(Token(token_type, lexeme))\n",
        "                            break\n",
        "                    if not match:\n",
        "                        print(f\"Lexical error at line {line_number + 1}: {line[0]}\")\n",
        "                        line = line[1:]\n",
        "\n",
        "    patterns = {\n",
        "        r'\\d+': TokenType.IntegerLiteral,\n",
        "        r'true|false': TokenType.BooleanLiteral,\n",
        "        r'[a-zA-Z][a-zA-Z0-9]*': TokenType.Identifier,\n",
        "        r'[\\+\\-\\*/=<>!]+': TokenType.Operator,\n",
        "        r'if|else|print': TokenType.Keyword,\n",
        "        r'\\/\\/.*$': TokenType.Comment,\n",
        "        r'\"[^\"]*\"': TokenType.StringLiteral  # Pattern for string literals\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    filename = input(\"Enter the filename: \")\n",
        "    scanner = Scanner(filename)\n",
        "    scanner.scan()\n",
        "    for token in scanner.tokens:\n",
        "        print(f\"{token.token_type}: {token.lexeme}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "dHqkArKF4hT_"
      }
    }
  ]
}